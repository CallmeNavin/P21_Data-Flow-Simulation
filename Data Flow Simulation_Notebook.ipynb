{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "l4c8GOvkD-Nn",
        "outputId": "7f9a04d1-3aaa-4400-af7d-ebb469998930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample datasets created successfully in 'Weekly_Uploaded/' folder!\n",
            "Accounting_Data_2025W41.csv  MES_Data_2025W41.csv\n",
            "Logistics_Data_2025W41.csv   Operational_Data_2025W41.csv\n",
            "Validation_Log.csv already exists\n",
            "['Accounting_Data', 'Logistics_Data', 'Operational_Data', 'MES_Data']\n",
            "Enter your name and department: Navin\n",
            "All files detected. Proceeding to validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Merge Successfully -> Master_Data_All.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9de05de-3af8-4def-a791-7c6dcc27bca2\", \"Master_Data_All.csv\", 1394)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd94a592-9e52-4b41-a2bd-d865ec6e3b87\", \"Validation_Log.csv\", 1529)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# VERSION 1: DATA INTEGRITY SYSTEM DESIGN & DATA CONSISTENCY, ROOT CAUSE SIMULATION\n",
        "\n",
        "# A. Create sample dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "os.makedirs(\"Weekly_Uploaded\", exist_ok=True)\n",
        "\n",
        "operational = pd.DataFrame({\"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"date\": [\"2025-10-01\", \"2025-10-02\", \"2025-10-03\"],\n",
        "    \"shift\": [1, 2, 1],\n",
        "    \"egg_weight_g\": [20.5, 21.1, 20.8],\n",
        "    \"larvae_age_day\": [5, 5, 6],\n",
        "    \"larvae_weight_kg\": [400, 420, 410],\n",
        "    \"feed_intake_kg\": [250, 260, 255],\n",
        "    \"mortality_rate\": [0.03, 0.02, 0.04],\n",
        "    \"drying_temp_avg\": [105, 107, 102],\n",
        "    \"drying_duration_min\": [180, 175, 185],\n",
        "    \"moisture_after_dry\": [9.2, 8.8, 9.5],\n",
        "    \"output_kg\": [320, 350, 300],\n",
        "    \"defect_kg\": [2, 1, 3],\n",
        "    \"operator\": [\"Anh\", \"Bình\", \"Cường\"]\n",
        "})\n",
        "operational.to_csv(\"Weekly_Uploaded/Operational_Data_2025W41.csv\", index=False)\n",
        "\n",
        "mes = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"sensor_id\": [\"S01\", \"S02\", \"S03\"],\n",
        "    \"avg_temp\": [105.5, 107.1, 101.8],\n",
        "    \"humidity\": [45, 50, 48],\n",
        "    \"drying_time_min\": [182, 174, 187],\n",
        "    \"chamber_id\": [\"C1\", \"C2\", \"C3\"],\n",
        "    \"vibration_alert\": [0, 0, 1]\n",
        "})\n",
        "mes.to_csv(\"Weekly_Uploaded/MES_Data_2025W41.csv\", index=False)\n",
        "\n",
        "accounting = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"feed_cost\": [120, 125, 118],\n",
        "    \"electricity_cost\": [50, 48, 55],\n",
        "    \"maintenance_cost\": [15, 18, 16],\n",
        "    \"labor_cost\": [45, 47, 46],\n",
        "    \"packaging_cost\": [10, 12, 11],\n",
        "    \"total_cost\": [240, 250, 246],\n",
        "    \"cost_date\": [\"2025-10-03\", \"2025-10-04\", \"2025-10-05\"]\n",
        "})\n",
        "accounting.to_csv(\"Weekly_Uploaded/Accounting_Data_2025W41.csv\", index=False)\n",
        "\n",
        "logistics = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"truck_id\": [\"T01\", \"T02\", \"T03\"],\n",
        "    \"shipment_date\": [\"2025-10-04\", \"2025-10-05\", \"2025-10-06\"],\n",
        "    \"destination\": [\"HCMC\", \"Dong Nai\", \"Ba Ria\"],\n",
        "    \"delivery_status\": [\"Delivered\", \"Delivered\", \"Delayed\"],\n",
        "    \"weight_kg\": [320, 350, 300],\n",
        "    \"delay_hour\": [0, 0, 12],\n",
        "    \"driver_name\": [\"Hưng\", \"Đạt\", \"Tâm\"]\n",
        "})\n",
        "logistics.to_csv(\"Weekly_Uploaded/Logistics_Data_2025W41.csv\", index=False)\n",
        "print(\"✅ Sample datasets created successfully in 'Weekly_Uploaded/' folder!\")\n",
        "\n",
        "!ls Weekly_Uploaded\n",
        "\n",
        "# B. Coding\n",
        "\n",
        "# 1. File Detection & Initialization\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "validation_log_path = os.path.join('Validation_Log.csv')\n",
        "if not os.path.exists(validation_log_path):\n",
        "  log_df = pd.DataFrame(columns= ['File_Name', 'File_Type', 'Upload_Date', 'Upload_By', 'Status', 'Error_Message'])\n",
        "  log_df.to_csv('Validation_Log.csv', index= False)\n",
        "  print('Created Validation_Log.csv')\n",
        "else:\n",
        "  print('Validation_Log.csv already exists')\n",
        "\n",
        "# 2. File Validation\n",
        "files = os.listdir('Weekly_Uploaded')\n",
        "required_files = ['Operational_Data', 'MES_Data', 'Accounting_Data', 'Logistics_Data']\n",
        "detected_files = [\"_\".join(r.split(\"_\")[:2]) for r in files]\n",
        "print(detected_files)\n",
        "missing = [r for r in required_files if r not in detected_files]\n",
        "\n",
        "uploader = input('Enter your name and department: ')\n",
        "vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "if missing:\n",
        "  log_entry = {'File_Name': ', '.join(missing), 'File_Type': '/ '.join(missing), 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': f'Missing: {missing}'}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  print(f'Missing files: {missing}. Process stopped')\n",
        "  raise SystemExit\n",
        "else:\n",
        "  print('All files detected. Proceeding to validation')\n",
        "\n",
        "import re\n",
        "expected_schema = {\"Operational_Data\": [\"batch_id\", \"date\", \"shift\", \"egg_weight_g\", \"larvae_age_day\", \"larvae_weight_kg\", \"feed_intake_kg\", \"mortality_rate\", \"drying_temp_avg\", \"drying_duration_min\", \"moisture_after_dry\", \"output_kg\", \"defect_kg\", \"operator\"], \"MES_Data\": [\"batch_id\", \"sensor_id\", \"avg_temp\", \"humidity\", \"drying_time_min\", \"chamber_id\", \"vibration_alert\"], \"Accounting_Data\": [\"batch_id\", \"feed_cost\", \"electricity_cost\", \"maintenance_cost\", \"labor_cost\", \"packaging_cost\", \"total_cost\", \"cost_date\"], \"Logistics_Data\": [\"batch_id\", \"truck_id\", \"shipment_date\", \"destination\", \"delivery_status\", \"weight_kg\", \"delay_hour\", \"driver_name\"]}\n",
        "pattern = r'_\\d{4}W\\d{2}\\.csv$'\n",
        "current_year = datetime.now().year\n",
        "current_week = datetime.now().isocalendar().week\n",
        "week_str = f'{current_year}W{current_week:02d}'\n",
        "for f in os.listdir('Weekly_Uploaded'):\n",
        "  if not f.endswith('.csv'):\n",
        "    log_entry = {'File_Name': f, 'File_Type': 'Unknown', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': 'File is not csv format'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "    print(f'Skipped. Not a csv file')\n",
        "    continue\n",
        "  if not re.search(pattern, f):\n",
        "    new_name = f'{f.split('.')[0]}_{week_str}.csv'\n",
        "    os.rename(os.path.join('Weekly_Uploaded', f), os.path.join('Weekly_Uploaded', new_name))\n",
        "    f = new_name\n",
        "    log_entry = {'File_Name': f, 'File_Type': 'Auto renamed', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Warning', 'Error_Message': 'File renamed to match naming convention'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode='a', index=False, header=False)\n",
        "\n",
        "  file_type = '_'.join(f.split('_')[:2])\n",
        "  df = pd.read_csv(os.path.join('Weekly_Uploaded', f))\n",
        "  expected_cols = expected_schema.get(file_type, [])\n",
        "  missing_cols = [c for c in expected_cols if c not in df.columns]\n",
        "\n",
        "  if missing_cols:\n",
        "    log_entry = {'File_Name': f, 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': f'Missing: {missing_cols}'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode = 'a', index= False, header= False)\n",
        "    print(f'Skipped. Missing column {missing_cols}')\n",
        "    continue\n",
        "  else:\n",
        "    print('Passed schema validation')\n",
        "\n",
        "# 3. Data Cleaning\n",
        "import numpy as np\n",
        "cleaned_dir = 'Clean_Data'\n",
        "os.makedirs(cleaned_dir, exist_ok= True)\n",
        "\n",
        "def cleaned_file(file_path, file_type):\n",
        "  df = pd.read_csv(file_path)\n",
        "  df.columns = df.columns.str.strip().str.lower()\n",
        "  df.dropna(how= 'all', inplace= True)\n",
        "  for col in df.columns:\n",
        "    if 'date' in col:\n",
        "      df[col] = pd.to_datetime(df[col], errors= 'coerce')\n",
        "    elif df[col].dtype == 'object':\n",
        "      df[col] = df[col].astype(str).str.strip()\n",
        "  numeric_col = df.select_dtypes(include= np.number).columns\n",
        "  for col in numeric_col:\n",
        "    q1 = df[col].quantile(0.25)\n",
        "    q3 = df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    outliers = (df[col] < q1 - 1.5 * iqr)|(df[col] > q3 + 1.5 * iqr)\n",
        "    df.loc[outliers, f'{col}_flag'] = 'Outlier'\n",
        "  cleaned_path = os.path.join(cleaned_dir, os.path.basename(file_path).replace('.csv', '_cleaned.csv'))\n",
        "  df.to_csv(cleaned_path, index= False)\n",
        "  log_entry = {'File_Name': os.path.basename(cleaned_path), 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Cleaned', 'Error_Message': 'Outliers flagged' if any(df.filter(like='_flag').count()) else 'OK'}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  return cleaned_path\n",
        "\n",
        "# 4. Append Clean Data\n",
        "master_dir = 'Master_Data'\n",
        "os.makedirs(master_dir, exist_ok= True)\n",
        "\n",
        "def append_to_master(cleaned_path, file_type):\n",
        "  master_file = os.path.join(master_dir, f'Master_{file_type.split('_')[0]}.csv')\n",
        "  cleaned_df = pd.read_csv(cleaned_path)\n",
        "  if os.path.exists(master_file):\n",
        "    master_df = pd.read_csv(master_file)\n",
        "    combined_df = pd.concat([master_df, cleaned_df], ignore_index= True)\n",
        "  else:\n",
        "    combined_df = cleaned_df\n",
        "  combined_df.drop_duplicates(subset= ['batch_id'], keep= 'last', inplace= True)\n",
        "  combined_df.to_csv(master_file, index= False)\n",
        "  log_entry = {'File_Name': os.path.basename(master_file), 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Appended to Master', 'Error_Message': ''}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "\n",
        "# 5. Merge Master Files\n",
        "def merge_all_master():\n",
        "  try:\n",
        "    operational = pd.read_csv(os.path.join(master_dir, 'Master_Operational.csv'))\n",
        "    mes = pd.read_csv(os.path.join(master_dir, 'Master_MES.csv'))\n",
        "    accounting = pd.read_csv(os.path.join(master_dir, 'Master_Accounting.csv'))\n",
        "    logistics = pd.read_csv(os.path.join(master_dir, 'Master_Logistics.csv'))\n",
        "    for df in [operational, mes, accounting, logistics]:\n",
        "      df.drop_duplicates(subset= ['batch_id'], keep= 'last', inplace= True)\n",
        "    merged = operational.merge(mes, on= 'batch_id', how= 'left')\\\n",
        "    .merge(accounting, on= 'batch_id', how= 'left')\\\n",
        "    .merge(logistics, on= 'batch_id', how= 'left')\n",
        "    merged.to_csv('Master_Data_All.csv', index= False)\n",
        "    print('Merge Successfully -> Master_Data_All.csv')\n",
        "    log_entry = {'File_Name': 'Master_Data_All.csv', 'File_Type': 'Merged Dataset', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Merged', 'Error_Message': ''}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  except Exception as e:\n",
        "    log_entry = {'File_Name': 'Master_Data_All.csv', 'File_Type': 'Merged Dataset', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': str(e)}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "    print(f'Merge Failed: {e}')\n",
        "\n",
        "for f in os.listdir('Weekly_Uploaded'):\n",
        "  file_type = '_'.join(f.split('_')[:2])\n",
        "  path = os.path.join('Weekly_Uploaded', f)\n",
        "  if file_type in expected_schema.keys():\n",
        "    cleaned_path = cleaned_file(path, file_type)\n",
        "    append_to_master(cleaned_path, file_type)\n",
        "merge_all_master()\n",
        "\n",
        "# 6. Check result\n",
        "from google.colab import files\n",
        "files.download('Master_Data_All.csv')\n",
        "files.download('Validation_Log.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERSION 2: ROOT CAUSE SIMULATION\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "files.upload()\n",
        "\n",
        "## Define acceptable range\n",
        "df = pd.read_csv('Master_Data_All.csv')\n",
        "df['yield_pct'] = df['output_kg'] / df['larvae_weight_kg']\n",
        "mean_yield = df['yield_pct'].mean()\n",
        "std_yield = df['yield_pct'].std()\n",
        "df['expected_low'] = mean_yield - 1.5 * std_yield\n",
        "df['expected_high'] = mean_yield + 1.5 * std_yield\n",
        "\n",
        "## Apply rule-based conditions\n",
        "conditions = [(df['yield_pct'] < df['expected_low']) | (df['yield_pct'] > df['expected_high']), (df['mortality_rate'] > 0.05), (df[\"drying_temp_avg\"] > 110) | (df[\"drying_temp_avg\"] < 95), (df[\"drying_duration_min\"] > 190) | (df[\"drying_duration_min\"] < 160), (df[\"moisture_after_dry\"] > 10), (df[\"defect_kg\"] > 3), (df[\"delivery_status\"] == \"Delayed\")]\n",
        "reasons = ['Yield out of range', 'High mortality rate', 'Abnormal drying temperature', 'Abnormal drying duration', 'High moisture after dry', 'Excessive defect weight', 'Delivery delayed']\n",
        "\n",
        "root_cause_list = []\n",
        "for i, c in enumerate(conditions):\n",
        "  sub = df.loc[c, [\"batch_id\", \"yield_pct\", \"mortality_rate\", \"drying_temp_avg\", \"drying_duration_min\", \"moisture_after_dry\", \"defect_kg\", \"delivery_status\"]].copy()\n",
        "  sub['Root_Cause'] = reasons[i]\n",
        "  root_cause_list.append(sub)\n",
        "\n",
        "## Generate reports\n",
        "if root_cause_list:\n",
        "  report = pd.concat(root_cause_list, ignore_index= True)\n",
        "else:\n",
        "  report = pd.DataFrame(columns= ['batch_id', 'Root_Cause'])\n",
        "report.drop_duplicates(subset= ['batch_id', 'Root_Cause'], inplace= True)\n",
        "report.to_csv('Root_Cause_Report.csv', index= False)\n",
        "print('Root Cause Report generated successfully')\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "vn_tz= pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "\n",
        "## Log activity to Validation_Log.csv\n",
        "log_entry = {\"File_Name\": \"RootCause_Report.csv\", \"File_Type\": \"Root Cause Analysis\", \"Upload_Date\": datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), \"Upload_By\": \"Vy – Data Analyst\", \"Status\": \"Generated\", \"Error_Message\": \"\"}\n",
        "pd.DataFrame([log_entry]).to_csv('Validation_Log.csv',mode= 'a', index= False, header= False)\n",
        "\n",
        "summary = report['Root_Cause'].value_counts().reset_index()\n",
        "summary.columns = ['Root_Cause', 'Count']\n",
        "summary.to_csv('Root_Cause_Summary.csv', index= False)\n",
        "files.download('Root_Cause_Report.csv')\n",
        "files.download('Root_Cause_Summary.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "PefVV2ob4ZCZ",
        "outputId": "70db39a3-7a32-46c8-dea5-cae1ff106078"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-187116a6-425b-4db5-8fc2-b0c4ded19cc8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-187116a6-425b-4db5-8fc2-b0c4ded19cc8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Master_Data_All.csv to Master_Data_All.csv\n",
            "Root Cause Report generated successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc7d63bd-44d6-4042-a837-80b9071aaaa9\", \"Root_Cause_Report.csv\", 194)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6f7ca9a-e300-41ff-b16c-6b993b87509c\", \"Root_Cause_Summary.csv\", 36)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERSION 1: DATA INTEGRITY SYSTEM DESIGN & DATA CONSISTENCY, ROOT CAUSE SIMULATION**\n",
        "\n",
        "_**1. File Detection & Initialization**_\n",
        "\n",
        "- Monitor folder /Weekly_Uploaded/ for new .csv files.\n",
        "- Validate presence of all 4 required domains (Operational_Data, MES_Data, Accounting_Data, Logistics_Data).\n",
        "- Auto-create log file Validation_Log.csv if missing.\n",
        "- Naming pattern enforced: <FileType>_YYYYW##.csv → e.g. MES_Data_2025W41.csv.\n",
        "- If fewer than 4 files detected → stop pipeline & log “Incomplete Upload” status.\n",
        "\n",
        "_**2. File Validation**_\n",
        "\n",
        "- Each file passes through schema and format validation:\n",
        "  + Check file type: ensure .csv format, rename automatically if needed.\n",
        "  + Check schema: compare columns against expected templates.\n",
        "  + Log errors to Validation_Log.csv if missing columns or mismatched schema.\n",
        "\n",
        "| File Type   | Expected Columns                                                                                                                                                                                |\n",
        "| ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Operational | batch_id, date, shift, egg_weight_g, larvae_age_day, larvae_weight_kg, feed_intake_kg, mortality_rate, drying_temp_avg, drying_duration_min, moisture_after_dry, output_kg, defect_kg, operator |\n",
        "| MES         | batch_id, sensor_id, avg_temp, humidity, drying_time_min, chamber_id, vibration_alert                                                                                                           |\n",
        "| Accounting  | batch_id, feed_cost, electricity_cost, maintenance_cost, labor_cost, packaging_cost, total_cost, cost_date                                                                                      |\n",
        "| Logistics   | batch_id, truck_id, shipment_date, destination, delivery_status, weight_kg, delay_hour, driver_name                                                                                             |\n",
        "\n",
        "_**3. Data Cleaning**_\n",
        "\n",
        "- Normalize column names → lowercase & trim spaces.\n",
        "- Remove blank rows.\n",
        "- Standardize data types (date, numeric, string).\n",
        "- Detect outliers using IQR method (Interquartile Range).\n",
        "- Export cleaned files → /Clean_Data/.\n",
        "- Output: [FileType]_cleaned.csv\n",
        "\n",
        "_**4. Master Data Appending**_\n",
        "\n",
        "- Each cleaned dataset is appended to its corresponding master dataset:\n",
        "\n",
        "| Cleaned File                 | Master Target          |\n",
        "| ---------------------------- | ---------------------- |\n",
        "| Operational_Data_cleaned.csv | Master_Operational.csv |\n",
        "| MES_Data_cleaned.csv         | Master_MES.csv         |\n",
        "| Accounting_Data_cleaned.csv  | Master_Accounting.csv  |\n",
        "| Logistics_Data_cleaned.csv   | Master_Logistics.csv   |\n",
        "\n",
        "- Remove duplicates by batch_id (keep latest).\n",
        "- Log results: “Appended to Master”.\n",
        "\n",
        "_**5. Master Merging**_\n",
        "\n",
        "- Merge all 4 master files on batch_id (LEFT JOIN) to create one consolidated dataset: Operational + MES + Accounting + Logistics → Master_Data_All.csv\n",
        "- Output: Master_Data_All.csv — base for analytical dashboards.\n",
        "\n",
        "_**6. Weekly Validation Log & Escalation**_\n",
        "\n",
        "- All operations (validation, cleaning, merging) are logged into Validation_Log.csv with timestamp and status.\n",
        "- This ensures traceability for future audits or automation."
      ],
      "metadata": {
        "id": "uG1fytrdHXwB"
      }
    }
  ]
}