{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "l4c8GOvkD-Nn",
        "outputId": "7f9a04d1-3aaa-4400-af7d-ebb469998930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample datasets created successfully in 'Weekly_Uploaded/' folder!\n",
            "Accounting_Data_2025W41.csv  MES_Data_2025W41.csv\n",
            "Logistics_Data_2025W41.csv   Operational_Data_2025W41.csv\n",
            "Validation_Log.csv already exists\n",
            "['Accounting_Data', 'Logistics_Data', 'Operational_Data', 'MES_Data']\n",
            "Enter your name and department: Navin\n",
            "All files detected. Proceeding to validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Passed schema validation\n",
            "Merge Successfully -> Master_Data_All.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9de05de-3af8-4def-a791-7c6dcc27bca2\", \"Master_Data_All.csv\", 1394)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd94a592-9e52-4b41-a2bd-d865ec6e3b87\", \"Validation_Log.csv\", 1529)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# VERSION 1: DATA INTEGRITY SYSTEM DESIGN & DATA CONSISTENCY, ROOT CAUSE SIMULATION\n",
        "\n",
        "# A. Create sample dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "os.makedirs(\"Weekly_Uploaded\", exist_ok=True)\n",
        "\n",
        "operational = pd.DataFrame({\"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"date\": [\"2025-10-01\", \"2025-10-02\", \"2025-10-03\"],\n",
        "    \"shift\": [1, 2, 1],\n",
        "    \"egg_weight_g\": [20.5, 21.1, 20.8],\n",
        "    \"larvae_age_day\": [5, 5, 6],\n",
        "    \"larvae_weight_kg\": [400, 420, 410],\n",
        "    \"feed_intake_kg\": [250, 260, 255],\n",
        "    \"mortality_rate\": [0.03, 0.02, 0.04],\n",
        "    \"drying_temp_avg\": [105, 107, 102],\n",
        "    \"drying_duration_min\": [180, 175, 185],\n",
        "    \"moisture_after_dry\": [9.2, 8.8, 9.5],\n",
        "    \"output_kg\": [320, 350, 300],\n",
        "    \"defect_kg\": [2, 1, 3],\n",
        "    \"operator\": [\"Anh\", \"Bình\", \"Cường\"]\n",
        "})\n",
        "operational.to_csv(\"Weekly_Uploaded/Operational_Data_2025W41.csv\", index=False)\n",
        "\n",
        "mes = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"sensor_id\": [\"S01\", \"S02\", \"S03\"],\n",
        "    \"avg_temp\": [105.5, 107.1, 101.8],\n",
        "    \"humidity\": [45, 50, 48],\n",
        "    \"drying_time_min\": [182, 174, 187],\n",
        "    \"chamber_id\": [\"C1\", \"C2\", \"C3\"],\n",
        "    \"vibration_alert\": [0, 0, 1]\n",
        "})\n",
        "mes.to_csv(\"Weekly_Uploaded/MES_Data_2025W41.csv\", index=False)\n",
        "\n",
        "accounting = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"feed_cost\": [120, 125, 118],\n",
        "    \"electricity_cost\": [50, 48, 55],\n",
        "    \"maintenance_cost\": [15, 18, 16],\n",
        "    \"labor_cost\": [45, 47, 46],\n",
        "    \"packaging_cost\": [10, 12, 11],\n",
        "    \"total_cost\": [240, 250, 246],\n",
        "    \"cost_date\": [\"2025-10-03\", \"2025-10-04\", \"2025-10-05\"]\n",
        "})\n",
        "accounting.to_csv(\"Weekly_Uploaded/Accounting_Data_2025W41.csv\", index=False)\n",
        "\n",
        "logistics = pd.DataFrame({\n",
        "    \"batch_id\": [\"B001\", \"B002\", \"B003\"],\n",
        "    \"truck_id\": [\"T01\", \"T02\", \"T03\"],\n",
        "    \"shipment_date\": [\"2025-10-04\", \"2025-10-05\", \"2025-10-06\"],\n",
        "    \"destination\": [\"HCMC\", \"Dong Nai\", \"Ba Ria\"],\n",
        "    \"delivery_status\": [\"Delivered\", \"Delivered\", \"Delayed\"],\n",
        "    \"weight_kg\": [320, 350, 300],\n",
        "    \"delay_hour\": [0, 0, 12],\n",
        "    \"driver_name\": [\"Hưng\", \"Đạt\", \"Tâm\"]\n",
        "})\n",
        "logistics.to_csv(\"Weekly_Uploaded/Logistics_Data_2025W41.csv\", index=False)\n",
        "print(\"✅ Sample datasets created successfully in 'Weekly_Uploaded/' folder!\")\n",
        "\n",
        "!ls Weekly_Uploaded\n",
        "\n",
        "# B. Coding\n",
        "\n",
        "# 1. File Detection & Initialization\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "validation_log_path = os.path.join('Validation_Log.csv')\n",
        "if not os.path.exists(validation_log_path):\n",
        "  log_df = pd.DataFrame(columns= ['File_Name', 'File_Type', 'Upload_Date', 'Upload_By', 'Status', 'Error_Message'])\n",
        "  log_df.to_csv('Validation_Log.csv', index= False)\n",
        "  print('Created Validation_Log.csv')\n",
        "else:\n",
        "  print('Validation_Log.csv already exists')\n",
        "\n",
        "# 2. File Validation\n",
        "files = os.listdir('Weekly_Uploaded')\n",
        "required_files = ['Operational_Data', 'MES_Data', 'Accounting_Data', 'Logistics_Data']\n",
        "detected_files = [\"_\".join(r.split(\"_\")[:2]) for r in files]\n",
        "print(detected_files)\n",
        "missing = [r for r in required_files if r not in detected_files]\n",
        "\n",
        "uploader = input('Enter your name and department: ')\n",
        "vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "if missing:\n",
        "  log_entry = {'File_Name': ', '.join(missing), 'File_Type': '/ '.join(missing), 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': f'Missing: {missing}'}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  print(f'Missing files: {missing}. Process stopped')\n",
        "  raise SystemExit\n",
        "else:\n",
        "  print('All files detected. Proceeding to validation')\n",
        "\n",
        "import re\n",
        "expected_schema = {\"Operational_Data\": [\"batch_id\", \"date\", \"shift\", \"egg_weight_g\", \"larvae_age_day\", \"larvae_weight_kg\", \"feed_intake_kg\", \"mortality_rate\", \"drying_temp_avg\", \"drying_duration_min\", \"moisture_after_dry\", \"output_kg\", \"defect_kg\", \"operator\"], \"MES_Data\": [\"batch_id\", \"sensor_id\", \"avg_temp\", \"humidity\", \"drying_time_min\", \"chamber_id\", \"vibration_alert\"], \"Accounting_Data\": [\"batch_id\", \"feed_cost\", \"electricity_cost\", \"maintenance_cost\", \"labor_cost\", \"packaging_cost\", \"total_cost\", \"cost_date\"], \"Logistics_Data\": [\"batch_id\", \"truck_id\", \"shipment_date\", \"destination\", \"delivery_status\", \"weight_kg\", \"delay_hour\", \"driver_name\"]}\n",
        "pattern = r'_\\d{4}W\\d{2}\\.csv$'\n",
        "current_year = datetime.now().year\n",
        "current_week = datetime.now().isocalendar().week\n",
        "week_str = f'{current_year}W{current_week:02d}'\n",
        "for f in os.listdir('Weekly_Uploaded'):\n",
        "  if not f.endswith('.csv'):\n",
        "    log_entry = {'File_Name': f, 'File_Type': 'Unknown', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': 'File is not csv format'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "    print(f'Skipped. Not a csv file')\n",
        "    continue\n",
        "  if not re.search(pattern, f):\n",
        "    new_name = f'{f.split('.')[0]}_{week_str}.csv'\n",
        "    os.rename(os.path.join('Weekly_Uploaded', f), os.path.join('Weekly_Uploaded', new_name))\n",
        "    f = new_name\n",
        "    log_entry = {'File_Name': f, 'File_Type': 'Auto renamed', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Warning', 'Error_Message': 'File renamed to match naming convention'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode='a', index=False, header=False)\n",
        "\n",
        "  file_type = '_'.join(f.split('_')[:2])\n",
        "  df = pd.read_csv(os.path.join('Weekly_Uploaded', f))\n",
        "  expected_cols = expected_schema.get(file_type, [])\n",
        "  missing_cols = [c for c in expected_cols if c not in df.columns]\n",
        "\n",
        "  if missing_cols:\n",
        "    log_entry = {'File_Name': f, 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': f'Missing: {missing_cols}'}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode = 'a', index= False, header= False)\n",
        "    print(f'Skipped. Missing column {missing_cols}')\n",
        "    continue\n",
        "  else:\n",
        "    print('Passed schema validation')\n",
        "\n",
        "# 3. Data Cleaning\n",
        "import numpy as np\n",
        "cleaned_dir = 'Clean_Data'\n",
        "os.makedirs(cleaned_dir, exist_ok= True)\n",
        "\n",
        "def cleaned_file(file_path, file_type):\n",
        "  df = pd.read_csv(file_path)\n",
        "  df.columns = df.columns.str.strip().str.lower()\n",
        "  df.dropna(how= 'all', inplace= True)\n",
        "  for col in df.columns:\n",
        "    if 'date' in col:\n",
        "      df[col] = pd.to_datetime(df[col], errors= 'coerce')\n",
        "    elif df[col].dtype == 'object':\n",
        "      df[col] = df[col].astype(str).str.strip()\n",
        "  numeric_col = df.select_dtypes(include= np.number).columns\n",
        "  for col in numeric_col:\n",
        "    q1 = df[col].quantile(0.25)\n",
        "    q3 = df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    outliers = (df[col] < q1 - 1.5 * iqr)|(df[col] > q3 + 1.5 * iqr)\n",
        "    df.loc[outliers, f'{col}_flag'] = 'Outlier'\n",
        "  cleaned_path = os.path.join(cleaned_dir, os.path.basename(file_path).replace('.csv', '_cleaned.csv'))\n",
        "  df.to_csv(cleaned_path, index= False)\n",
        "  log_entry = {'File_Name': os.path.basename(cleaned_path), 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Cleaned', 'Error_Message': 'Outliers flagged' if any(df.filter(like='_flag').count()) else 'OK'}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  return cleaned_path\n",
        "\n",
        "# 4. Append Clean Data\n",
        "master_dir = 'Master_Data'\n",
        "os.makedirs(master_dir, exist_ok= True)\n",
        "\n",
        "def append_to_master(cleaned_path, file_type):\n",
        "  master_file = os.path.join(master_dir, f'Master_{file_type.split('_')[0]}.csv')\n",
        "  cleaned_df = pd.read_csv(cleaned_path)\n",
        "  if os.path.exists(master_file):\n",
        "    master_df = pd.read_csv(master_file)\n",
        "    combined_df = pd.concat([master_df, cleaned_df], ignore_index= True)\n",
        "  else:\n",
        "    combined_df = cleaned_df\n",
        "  combined_df.drop_duplicates(subset= ['batch_id'], keep= 'last', inplace= True)\n",
        "  combined_df.to_csv(master_file, index= False)\n",
        "  log_entry = {'File_Name': os.path.basename(master_file), 'File_Type': file_type, 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Appended to Master', 'Error_Message': ''}\n",
        "  pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "\n",
        "# 5. Merge Master Files\n",
        "def merge_all_master():\n",
        "  try:\n",
        "    operational = pd.read_csv(os.path.join(master_dir, 'Master_Operational.csv'))\n",
        "    mes = pd.read_csv(os.path.join(master_dir, 'Master_MES.csv'))\n",
        "    accounting = pd.read_csv(os.path.join(master_dir, 'Master_Accounting.csv'))\n",
        "    logistics = pd.read_csv(os.path.join(master_dir, 'Master_Logistics.csv'))\n",
        "    for df in [operational, mes, accounting, logistics]:\n",
        "      df.drop_duplicates(subset= ['batch_id'], keep= 'last', inplace= True)\n",
        "    merged = operational.merge(mes, on= 'batch_id', how= 'left')\\\n",
        "    .merge(accounting, on= 'batch_id', how= 'left')\\\n",
        "    .merge(logistics, on= 'batch_id', how= 'left')\n",
        "    merged.to_csv('Master_Data_All.csv', index= False)\n",
        "    print('Merge Successfully -> Master_Data_All.csv')\n",
        "    log_entry = {'File_Name': 'Master_Data_All.csv', 'File_Type': 'Merged Dataset', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Merged', 'Error_Message': ''}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "  except Exception as e:\n",
        "    log_entry = {'File_Name': 'Master_Data_All.csv', 'File_Type': 'Merged Dataset', 'Upload_Date': datetime.now(vn_tz).strftime(\"%Y-%m-%d %H:%M:%S\"), 'Upload_By': uploader, 'Status': 'Failed', 'Error_Message': str(e)}\n",
        "    pd.DataFrame([log_entry]).to_csv(validation_log_path, mode= 'a', index= False, header= False)\n",
        "    print(f'Merge Failed: {e}')\n",
        "\n",
        "for f in os.listdir('Weekly_Uploaded'):\n",
        "  file_type = '_'.join(f.split('_')[:2])\n",
        "  path = os.path.join('Weekly_Uploaded', f)\n",
        "  if file_type in expected_schema.keys():\n",
        "    cleaned_path = cleaned_file(path, file_type)\n",
        "    append_to_master(cleaned_path, file_type)\n",
        "merge_all_master()\n",
        "\n",
        "# 6. Check result\n",
        "from google.colab import files\n",
        "files.download('Master_Data_All.csv')\n",
        "files.download('Validation_Log.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERSION 1: DATA INTEGRITY SYSTEM DESIGN & DATA CONSISTENCY, ROOT CAUSE SIMULATION**\n",
        "\n",
        "_**1. File Detection & Initialization**_\n",
        "\n",
        "- Monitor folder /Weekly_Uploaded/ for new .csv files.\n",
        "- Validate presence of all 4 required domains (Operational_Data, MES_Data, Accounting_Data, Logistics_Data).\n",
        "- Auto-create log file Validation_Log.csv if missing.\n",
        "- Naming pattern enforced: <FileType>_YYYYW##.csv → e.g. MES_Data_2025W41.csv.\n",
        "- If fewer than 4 files detected → stop pipeline & log “Incomplete Upload” status.\n",
        "\n",
        "_**2. File Validation**_\n",
        "\n",
        "- Each file passes through schema and format validation:\n",
        "  + Check file type: ensure .csv format, rename automatically if needed.\n",
        "  + Check schema: compare columns against expected templates.\n",
        "  + Log errors to Validation_Log.csv if missing columns or mismatched schema.\n",
        "\n",
        "| File Type   | Expected Columns                                                                                                                                                                                |\n",
        "| ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Operational | batch_id, date, shift, egg_weight_g, larvae_age_day, larvae_weight_kg, feed_intake_kg, mortality_rate, drying_temp_avg, drying_duration_min, moisture_after_dry, output_kg, defect_kg, operator |\n",
        "| MES         | batch_id, sensor_id, avg_temp, humidity, drying_time_min, chamber_id, vibration_alert                                                                                                           |\n",
        "| Accounting  | batch_id, feed_cost, electricity_cost, maintenance_cost, labor_cost, packaging_cost, total_cost, cost_date                                                                                      |\n",
        "| Logistics   | batch_id, truck_id, shipment_date, destination, delivery_status, weight_kg, delay_hour, driver_name                                                                                             |\n",
        "\n",
        "_**3. Data Cleaning**_\n",
        "\n",
        "- Normalize column names → lowercase & trim spaces.\n",
        "- Remove blank rows.\n",
        "- Standardize data types (date, numeric, string).\n",
        "- Detect outliers using IQR method (Interquartile Range).\n",
        "- Export cleaned files → /Clean_Data/.\n",
        "- Output: [FileType]_cleaned.csv\n",
        "\n",
        "_**4. Master Data Appending**_\n",
        "\n",
        "- Each cleaned dataset is appended to its corresponding master dataset:\n",
        "\n",
        "| Cleaned File                 | Master Target          |\n",
        "| ---------------------------- | ---------------------- |\n",
        "| Operational_Data_cleaned.csv | Master_Operational.csv |\n",
        "| MES_Data_cleaned.csv         | Master_MES.csv         |\n",
        "| Accounting_Data_cleaned.csv  | Master_Accounting.csv  |\n",
        "| Logistics_Data_cleaned.csv   | Master_Logistics.csv   |\n",
        "\n",
        "- Remove duplicates by batch_id (keep latest).\n",
        "- Log results: “Appended to Master”.\n",
        "\n",
        "_**5. Master Merging**_\n",
        "\n",
        "- Merge all 4 master files on batch_id (LEFT JOIN) to create one consolidated dataset: Operational + MES + Accounting + Logistics → Master_Data_All.csv\n",
        "- Output: Master_Data_All.csv — base for analytical dashboards.\n",
        "\n",
        "_**6. Weekly Validation Log & Escalation**_\n",
        "\n",
        "- All operations (validation, cleaning, merging) are logged into Validation_Log.csv with timestamp and status.\n",
        "- This ensures traceability for future audits or automation."
      ],
      "metadata": {
        "id": "uG1fytrdHXwB"
      }
    }
  ]
}